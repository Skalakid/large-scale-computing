{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executed commands\n",
    "1. cd $SCRATCH\n",
    "2. module load cuda/12.1.1\n",
    "3. conda create --prefix $SCRATCH/a1 python=3.11 -y\n",
    "4. conda activate a1/\n",
    "5. conda install numpy\n",
    "6. conda install -c conda-forge cupy\n",
    "7. conda install -c conda-forge jupyter\n",
    "8. python -m ipykernel install --user --name=a1 --display-name \"cuda-a1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "# Load and preprocess the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2489.7211369835995\n",
      "CPU time: 30.31 seconds\n",
      "2502.0647\n",
      "GPU time: 0.16 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Matrix size\n",
    "N = 10000\n",
    "\n",
    "def run_experiment(N):\n",
    "    # CPU-based matrix multiplication using NumPy\n",
    "    start_time = time.time()\n",
    "    A_cpu = np.random.rand(N, N)\n",
    "    B_cpu = np.random.rand(N, N)\n",
    "    C_cpu = np.dot(A_cpu, B_cpu)\n",
    "    print(C_cpu[1][1])\n",
    "    cpu_time = time.time() - start_time\n",
    "    print(f\"CPU time: {cpu_time:.2f} seconds\")\n",
    "\n",
    "    # GPU-based matrix multiplication using CuPy\n",
    "    start_time = time.time()\n",
    "    A_gpu = cp.random.rand(N, N, dtype=cp.float32)\n",
    "    B_gpu = cp.random.rand(N, N, dtype=cp.float32)\n",
    "    C_gpu = cp.dot(A_gpu, B_gpu)\n",
    "    print(C_gpu[1][1])\n",
    "    gpu_time = time.time() - start_time\n",
    "    print(f\"GPU time: {gpu_time:.2f} seconds\")\n",
    "\n",
    "run_experiment(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2475.867684654416\n",
    "CPU time: 30.47 seconds\n",
    "2500.9556\n",
    "GPU time: 2.66 seconds\n",
    "\n",
    "2506.9204726489556\n",
    "CPU time: 29.92 seconds\n",
    "2488.008\n",
    "GPU time: 0.16 seconds\n",
    "\n",
    "2477.12437561877\n",
    "CPU time: 30.30 seconds\n",
    "2495.3054\n",
    "GPU time: 0.15 seconds\n",
    "\n",
    "2522.5097704869895\n",
    "CPU time: 30.17 seconds\n",
    "2491.4421\n",
    "GPU time: 0.16 seconds\n",
    "\n",
    "2493.952022696899\n",
    "CPU time: 30.20 seconds\n",
    "2492.9202\n",
    "GPU time: 0.16 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "matrix_size: 1000\n",
      "254.5997635655197\n",
      "CPU time: 0.05 seconds\n",
      "238.40604\n",
      "GPU time: 0.00 seconds\n",
      "\n",
      "249.3503072999922\n",
      "CPU time: 0.05 seconds\n",
      "240.22192\n",
      "GPU time: 0.00 seconds\n",
      "\n",
      "248.10468720154867\n",
      "CPU time: 0.05 seconds\n",
      "231.59483\n",
      "GPU time: 0.00 seconds\n",
      "\n",
      "---------------\n",
      "matrix_size: 3000\n",
      "756.2997635100163\n",
      "CPU time: 0.95 seconds\n",
      "734.2489\n",
      "GPU time: 0.01 seconds\n",
      "\n",
      "759.9756431413139\n",
      "CPU time: 0.94 seconds\n",
      "748.5583\n",
      "GPU time: 0.00 seconds\n",
      "\n",
      "747.5341773560866\n",
      "CPU time: 0.94 seconds\n",
      "748.78424\n",
      "GPU time: 0.01 seconds\n",
      "\n",
      "---------------\n",
      "matrix_size: 5000\n",
      "1259.1085296551275\n",
      "CPU time: 4.10 seconds\n",
      "1214.0638\n",
      "GPU time: 0.02 seconds\n",
      "\n",
      "1241.2225234763455\n",
      "CPU time: 4.09 seconds\n",
      "1256.4404\n",
      "GPU time: 0.02 seconds\n",
      "\n",
      "1259.3081245589249\n",
      "CPU time: 4.10 seconds\n",
      "1273.2544\n",
      "GPU time: 0.02 seconds\n",
      "\n",
      "---------------\n",
      "matrix_size: 10000\n",
      "2519.5522328586467\n",
      "CPU time: 30.29 seconds\n",
      "2478.8318\n",
      "GPU time: 0.16 seconds\n",
      "\n",
      "2526.6076206243533\n",
      "CPU time: 30.30 seconds\n",
      "2507.1755\n",
      "GPU time: 0.16 seconds\n",
      "\n",
      "2445.6647272436644\n",
      "CPU time: 30.29 seconds\n",
      "2505.9636\n",
      "GPU time: 0.16 seconds\n",
      "\n",
      "---------------\n",
      "matrix_size: 20000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "N_sizes = [1000, 3000, 5000, 10000, 20000]\n",
    "\n",
    "for matrix_size in N_sizes:\n",
    "    print(\"---------------\")\n",
    "    print(f\"matrix_size: {matrix_size}\")\n",
    "    for i in range(3):\n",
    "        run_experiment(matrix_size)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Small Matrices (N=1000)\n",
    "\n",
    "The CPU might be nearly as fast as the GPU because the data transfer overhead to the GPU can dominate.\n",
    "\n",
    "GPU acceleration might not be significant at this scale.\n",
    "\n",
    "### For Medium Matrices (N=3000,5000)\n",
    "\n",
    "The GPU should start showing significant speedup as matrix size increases.\n",
    "\n",
    "CPU time will increase significantly due to higher computational complexity O(N^3)\n",
    "\n",
    "### For Large Matrices (N=10000,20000)\n",
    "\n",
    "The CPU execution time will become very large. Avg time 30.295 when previously it was lower than 5\n",
    "\n",
    "The GPU will show a dramatic speedup due to parallel processing. GPU time eqeals 0.16 which is very fast in this situation!\n",
    "\n",
    "GPU memory limitations might occur at very high values of N, requiring batch processing or optimized memory management. The kernel crashed when N was set to 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat GPT conversation link: https://chatgpt.com/share/67d721c2-28e0-8001-8b87-b5f647c7be5c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-a1",
   "language": "python",
   "name": "a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
